{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MNIST](http://yann.lecun.com/exdb/mnist/) para que nossa rede seja capaz de gerar dígitos escritos artificiais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_gan as tfgan\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#Fazer Gan na mao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "image, label = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='train', \n",
    "    batch_size=-1, \n",
    "    as_supervised=True,\n",
    "))\n",
    "print(type(label),label)\n",
    "print(type(image), image.shape)\n",
    "plt.imshow(image[0,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[1,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[2,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[3,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[4,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "train_image = image\n",
    "train_labels = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='test', \n",
    "    batch_size=-1, \n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "print(type(image), image.shape)\n",
    "plt.imshow(image[0,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[1,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[2,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[3,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(image[4,:,:,0],cmap=\"gray\")\n",
    "plt.figure()\n",
    "test_image = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_image.reshape((len(train_image),28,28))) / 255.\n",
    "test_dataset = (test_image.reshape((len(test_image),28,28))) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset[0,:,:],cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(test_dataset[1,:,:],cmap=\"gray\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, BatchNormalization,Reshape, Activation, Dense, Flatten, MaxPooling2D\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "\n",
    "embedding_dim = 16\n",
    "def _batch_norm(inputs, is_training):\n",
    "  return tf.layers.batch_normalization(\n",
    "      inputs, momentum=0.999, epsilon=0.001, training=is_training)\n",
    "\n",
    "#Generator\n",
    "def unconditional_generator(noise,  weight_decay=2.5e-5):\n",
    "    \"\"\"Generator to produce unconditional MNIST images.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=64, units=512))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(64*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((7, 7, 64), input_shape=(64*7*7,))) # 7x7 image\n",
    "    \n",
    "    model.add(UpSampling2D(size=(2, 2))) # 14x14 image\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(UpSampling2D(size=(2, 2))) # 28x28 image\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def unconditional_discriminator(img, unused_conditioning, weight_decay=2.5e-5):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(32,(5, 5),padding='same',input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (5, 5)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512))\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(1))\n",
    " \n",
    "    return model\n",
    "\n",
    "#Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_lr = 0.001 #@param\n",
    "discriminator_lr = 0.0002 #@param\n",
    "\n",
    "def gen_opt():\n",
    "  gstep = tf.train.get_or_create_global_step()\n",
    "  base_lr = generator_lr\n",
    "  # Halve the learning rate at 1000 steps.\n",
    "  lr = tf.cond(gstep < 1000, lambda: base_lr, lambda: base_lr / 2.0)\n",
    "  return tf.keras.optimizers.Adam(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_gan as tfgan\n",
    "import tensorflow as tf\n",
    "gan_estimator = tfgan.estimator.GANEstimator(     \n",
    "        generator_fn=unconditional_generator,\n",
    "        discriminator_fn=unconditional_discriminator,        \n",
    "        generator_loss_fn=tfgan.losses.modified_generator_loss,\n",
    "        discriminator_loss_fn=tfgan.losses.modified_discriminator_loss,        \n",
    "        generator_optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "        discriminator_optimizer=tf.keras.optimizers.Adam(0.0002, 0.5),\n",
    "        add_summaries=tfgan.estimator.SummaryType.IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here\n",
    "def input_fn():\n",
    "    return tf.convert_to_tensor(train_dataset)\n",
    "\n",
    "import time\n",
    "steps_per_eval = 500 #@param\n",
    "max_train_steps = 5000 #@param\n",
    "batches_for_eval_metrics = 100 #@param\n",
    "\n",
    "# Used to track metrics.\n",
    "steps = []\n",
    "real_logits, fake_logits = [], []\n",
    "real_mnist_scores, mnist_scores, frechet_distances = [], [], []\n",
    "\n",
    "cur_step = 0\n",
    "start_time = time.time()\n",
    "while cur_step < max_train_steps:\n",
    "  next_step = min(cur_step + steps_per_eval, max_train_steps)\n",
    "\n",
    "  start = time.time()\n",
    "  gan_estimator.train(input_fn, max_steps=next_step)\n",
    "  steps_taken = next_step - cur_step\n",
    "  time_taken = time.time() - start\n",
    "  print('Time since start: %.2f min' % ((time.time() - start_time) / 60.0))\n",
    "  print('Trained from step %i to %i in %.2f steps / sec' % (\n",
    "      cur_step, next_step, steps_taken / time_taken))\n",
    "  cur_step = next_step\n",
    "  \n",
    "  # Calculate some metrics.\n",
    "  metrics = gan_estimator.evaluate(input_fn, steps=batches_for_eval_metrics)\n",
    "  steps.append(cur_step)\n",
    "  real_logits.append(metrics['real_data_logits'])\n",
    "  fake_logits.append(metrics['gen_data_logits'])\n",
    "  real_mnist_scores.append(metrics['real_mnist_score'])\n",
    "  mnist_scores.append(metrics['mnist_score'])\n",
    "  frechet_distances.append(metrics['frechet_distance'])\n",
    "  print('Average discriminator output on Real: %.2f  Fake: %.2f' % (\n",
    "      real_logits[-1], fake_logits[-1]))\n",
    "  print('Inception Score: %.2f / %.2f  Frechet Distance: %.2f' % (\n",
    "      mnist_scores[-1], real_mnist_scores[-1], frechet_distances[-1]))\n",
    "  \n",
    "  # Vizualize some images.\n",
    "  iterator = gan_estimator.predict(\n",
    "      input_fn, hooks=[tf.train.StopAtStepHook(num_steps=21)])\n",
    "  try:\n",
    "    imgs = np.array([next(iterator) for _ in range(20)])\n",
    "  except StopIteration:\n",
    "    pass\n",
    "  tiled = tfgan.eval.python_image_grid(imgs, grid_shape=(2, 10))\n",
    "  plt.axis('off')\n",
    "  plt.imshow(np.squeeze(tiled))\n",
    "  plt.show()\n",
    "  \n",
    "  \n",
    "# Plot the metrics vs step.\n",
    "plt.title('MNIST Frechet distance per step')\n",
    "plt.plot(steps, frechet_distances)\n",
    "plt.figure()\n",
    "plt.title('MNIST Score per step')\n",
    "plt.plot(steps, mnist_scores)\n",
    "plt.plot(steps, real_mnist_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "def generator_cost_function(y_actual, y_predicted):\n",
    "        n_examples = y_actual.shape[0]\n",
    "        #batch = tf.convert_to_tensor(np.zeros((n_examples*2, 28, 28, 1)), dtype=tf.float32)\n",
    "        batch = tf.concat([y_actual, y_predicted], axis=0)\n",
    "        #batch[n_examples:, :, :, 0] = y_predicted\n",
    "        #Designa labels inversas para calcular a loss(o gerador tenta fazer o discriminador errar)\n",
    "        labels = np.zeros(batch.shape[0])\n",
    "        labels[n_examples:] = 1. #1 é a label dos exemplos reais para o discriminador e para os falsos no gerador\n",
    "        prediction = gan_discriminator.predict(batch)\n",
    "        bce = tf.keras.losses.BinaryCrossentropy(tf.convert_to_tensor(labels),prediction)\n",
    "        return bce\n",
    "    \n",
    "    \n",
    "\n",
    "dim_random_vector = 64\n",
    "gan_generator = gan_generator_model(dim_random_vector)\n",
    "gan_generator.compile(optimizer=Adam(0.0002), loss=generator_cost_function, metrics=['accuracy'])\n",
    "gan_generator.summary()\n",
    "\n",
    "gan_discriminator = gan_discriminator_model()\n",
    "gan_discriminator.compile(optimizer=Adam(0.0002), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "gan_discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "repet = 1000\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "for i in range(repet):\n",
    "       \n",
    "    \n",
    "    gan_generator.fit(np.random.rand(len(train_dataset),dim_random_vector),train_dataset, epochs = epochs,  verbose=2)\n",
    "    \n",
    "    batch = np.zeros((len(train_dataset)*2,28,28,1))\n",
    "    batch[:len(train_dataset), :, :, :] =train_dataset\n",
    "    batch[len(train_dataset):, :, :, :] = gan_generator.predict(np.random.rand(len(train_dataset),dim_random_vector))\n",
    "    labels = np.zeros(len(train_dataset)*2)\n",
    "    labels[:len(train_dataset)] = 1.\n",
    "    gan_discriminator.fit(batch, labels, epochs= epochs, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_generator_model(dim_random):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=512, input_dim=dim_random, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(64*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Reshape((7, 7, 64), input_shape=(64*7*7,))) # 7x7 image\n",
    "    \n",
    "    model.add(UpSampling2D(size=(2, 2))) # 14x14 image\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(UpSampling2D(size=(2, 2))) # 28x28 image\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    #model.add(Dense(28*28))\n",
    "    #model.add(Reshape((28, 28, 1)))\n",
    "    #model.add(Activation('relu'))\n",
    "    return model\n",
    "\n",
    "def gan_discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32,(5, 5),padding='same',input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(64, (5, 5)))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    " \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    #model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # connect generator and discriminator\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the discriminator\n",
    "    model.add(discriminator)\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "generator = gan_generator_model(dim_random_vector)\n",
    "generator.summary()\n",
    "discriminator = gan_discriminator_model()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "discriminator.summary()\n",
    "gan = define_gan(generator, discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.reshape((len(train_dataset),28,28,1))\n",
    "batch_size = 512\n",
    "half_batch = int(batch_size/2)\n",
    "epochs = 10000\n",
    "\n",
    "errors_discrim = np.zeros((epochs))\n",
    "errors_generator = np.zeros((epochs))\n",
    "\n",
    "plt.ion()\n",
    "# manually enumerate epochs\n",
    "for i in range(epochs):\n",
    "    batch = np.zeros((batch_size, 28, 28, 1))\n",
    "    batch_y = np.zeros(batch_size)\n",
    "    # prepare real samples\n",
    "    random_indices = np.random.choice(len(train_dataset), size=half_batch, replace=False)\n",
    "    batch[:half_batch, :, :, :] = train_dataset[random_indices, :, :,:]\n",
    "    #Label for the real samples is 0, so no need to change batch_y\n",
    "\n",
    "    batch[half_batch:, :, :] = generator.predict(np.random.rand(half_batch, dim_random_vector))\n",
    "    batch_y[half_batch:] += 1.\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print_images(batch[-25:])\n",
    "        \n",
    "   \n",
    "        \n",
    "    \n",
    "   \n",
    "    errors_discrim[i],_ = discriminator.train_on_batch(batch, batch_y)\n",
    "    # create inverted labels for the fake samples\n",
    "    x_gan = np.random.rand(batch_size, dim_random_vector)\n",
    "    y_gan = np.zeros((batch_size)) + 1.\n",
    "    # update the generator via the discriminator's error\n",
    "    errors_generator[i] = gan.train_on_batch(x_gan, y_gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(range(len(errors_discrim)),errors_discrim)\n",
    "plt.scatter(range(len(errors_generator)),errors_generator)\n",
    "plt.figure()\n",
    "\n",
    "generated = generator.predict(np.random.rand(50, dim_random_vector))\n",
    "plt.imshow(generated[0,:,:].reshape(28,28),cmap=\"gray\")\n",
    "plt.figure()\n",
    "plt.imshow(generated[1,:,:].reshape(28,28),cmap=\"gray\")\n",
    "plt.figure()\n",
    "\n",
    "print(sum(discriminator.predict(generated)))\n",
    "print(sum(discriminator.predict(train_dataset[:50, :, :, :])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(samples):\n",
    "    plt.close()\n",
    "    for i in range(25):\n",
    "        # define subplot\n",
    "        plt.subplot(5, 5, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(samples[i].reshape((28,28)), cmap='gray_r')\n",
    "    plt.show()\n",
    "    \n",
    "print_images(batch)\n",
    "generated = generator.predict(np.random.rand(50, dim_random_vector))\n",
    "print(batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Softmax\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Conv2DTranspose, LeakyReLU, Dropout, Input\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, BatchNormalization,Reshape, Activation, Dense, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='train', \n",
    "    batch_size=-1, \n",
    "    as_supervised=True,\n",
    "))\n",
    "train_image = image\n",
    "train_labels = label\n",
    "image, label = tfds.as_numpy(tfds.load(\n",
    "    'mnist',\n",
    "    split='test', \n",
    "    batch_size=-1, \n",
    "    as_supervised=True,\n",
    "))\n",
    "\n",
    "test_image = image\n",
    "test_labels = label\n",
    "\n",
    "train_dataset = (train_image.reshape((len(train_image),28,28))-127.5)/127.5\n",
    "test_dataset = (test_image.reshape((len(test_image),28,28))-127.5)/127.5\n",
    "train_dataset = train_dataset.reshape((len(train_dataset),28,28,1))\n",
    "test_dataset = test_dataset.reshape((len(test_dataset),28,28,1))\n",
    "\n",
    "dim_random_vector = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(samples):\n",
    "    plt.close()\n",
    "    for i in range(25):\n",
    "        # define subplot\n",
    "        plt.subplot(5, 5, 1 + i)\n",
    "        # turn off axis\n",
    "        plt.axis('off')\n",
    "        # plot raw pixel data\n",
    "        plt.imshow(samples[i].reshape((28,28)), cmap='gray_r')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def gan_generator_model(dim_random):\n",
    "    model = Sequential()\n",
    "    if True:\n",
    "        model.add(Dense(256, input_dim=dim_random, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Dense(512))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(1024))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Dense(28*28, activation='tanh'))\n",
    "        model.add(Reshape((28,28,1)))\n",
    "    else:\n",
    "        model.add(Dense(128*7*7, input_dim=dim_random, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(Reshape((7, 7, 128)))\n",
    "        model.add(UpSampling2D(size=(2, 2)))\n",
    "        model.add(Conv2D(64, kernel_size=(5, 5), padding='same'))\n",
    "        model.add(LeakyReLU(0.2))\n",
    "        model.add(UpSampling2D(size=(2, 2)))\n",
    "        model.add(Conv2D(1, kernel_size=(5, 5), padding='same', activation='tanh'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def gan_discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', input_shape=(28,28,1), kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, kernel_size=(5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # connect generator and discriminator\n",
    "    discriminator.trainable = False\n",
    "    ganInput = Input(shape=(dim_random_vector,))\n",
    "    x = generator(ganInput)\n",
    "    ganOutput = discriminator(x)\n",
    "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return gan\n",
    "\n",
    "\n",
    "generator = gan_generator_model(dim_random_vector)\n",
    "generator.summary()\n",
    "discriminator = gan_discriminator_model()\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "discriminator.summary()\n",
    "gan = define_gan(generator, discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "half_batch = int(batch_size/2)\n",
    "epochs = 5000\n",
    "\n",
    "test_size = 200\n",
    "half_test = int(test_size /2)\n",
    "\n",
    "errors_discrim = np.zeros((epochs))\n",
    "errors_generator = np.zeros((epochs))\n",
    "perc_discrim = np.zeros((epochs))\n",
    "\n",
    "plt.ion()\n",
    "# manually enumerate epochs\n",
    "for i in range(epochs):\n",
    "    batch = np.zeros((batch_size, 28, 28, 1))\n",
    "    batch_y = np.zeros(batch_size)\n",
    "    # prepare real samples\n",
    "    random_indices = np.random.choice(len(train_dataset), size=half_batch, replace=False)\n",
    "    batch[:half_batch, :, :, :] = train_dataset[random_indices, :, :,:]\n",
    "    #Label for the real samples is 0, so no need to change batch_y\n",
    "\n",
    "    batch[half_batch:, :, :] = generator.predict(np.random.normal(0, 1, size=[half_batch, dim_random_vector]))\n",
    "    batch_y[half_batch:] += 0.9 \n",
    "   \n",
    "        \n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    errors_discrim[i],_ = discriminator.train_on_batch(batch, batch_y)\n",
    "    # create inverted labels for the fake samples\n",
    "    x_gan = np.random.normal(0, 1, size=[batch_size, dim_random_vector])\n",
    "    y_gan = np.zeros((batch_size)) \n",
    "    discriminator.trainable = False\n",
    "    # update the generator via the discriminator's error\n",
    "    errors_generator[i] = gan.train_on_batch(x_gan, y_gan)\n",
    "    \n",
    "    test_batch = np.zeros((test_size, 28, 28, 1))\n",
    "    test_y     = np.zeros(test_size)\n",
    "    test_y[half_test:] += 1.\n",
    "    random_indices = np.random.choice(len(test_dataset), size=half_test, replace=False)\n",
    "\n",
    "    test_batch[:half_test, :, :, :] = test_dataset[random_indices, :, :]\n",
    "    test_batch[half_test:, :, :] = generator.predict(np.random.normal(0, 1, size=[half_test, dim_random_vector]))\n",
    "    predictions =  (discriminator.predict(test_batch).ravel() > 0.5) * 1.\n",
    "    \n",
    "    perc_discrim[i] = sum(predictions == test_y)/test_size\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(\"epoch: \"+str(i))\n",
    "        print_images(generator.predict(np.random.normal(0, 1, size=[25, dim_random_vector])))\n",
    "        print(\"Errors Discrimin: \" + str(errors_generator[i]))\n",
    "        print(\"Errors Generator:\" + str(errors_discrim[i]))\n",
    "        print(\"Perc Discrim:\" + str(perc_discrim[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors_discrim)\n",
    "\n",
    "plt.plot(errors_generator)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(perc_discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.normal(0,1, size=[2, dim_random_vector])\n",
    "samples = generator.predict(rnd)\n",
    "plt.imshow(samples[0].reshape((28,28)), cmap='gray_r')\n",
    "plt.figure()\n",
    "plt.imshow(samples[1].reshape((28,28)), cmap='gray_r')\n",
    "print(rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim= 5\n",
    "def real_function(x):\n",
    "    return np.multiply(x,x)\n",
    "\n",
    "def generate_samples(n_samples):\n",
    "    result = np.zeros((n_samples,2))\n",
    "    max_x,min_x = -1,+1\n",
    "    x = (np.random.rand(n_samples)*(max_x-min_x)) + min_x\n",
    "    result[:,0] = x\n",
    "    result[:,1] = real_function(x)\n",
    "    return result\n",
    "def generator_predict(n_samples, input_dim, generator):\n",
    "    rnd = np.random.normal(0, 1, size=[n_samples, input_dim])\n",
    "    return generator.predict(rnd)\n",
    "\n",
    "train_dataset = generate_samples(10000)\n",
    "test_dataset = generate_samples(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_images(input_v):\n",
    "    plt.close()\n",
    "    x = input_v[:,0]\n",
    "    y = input_v[:,1]\n",
    "    plt.scatter(x,y)\n",
    "    plt.show()\n",
    "def gan_generator_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_dim=input_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dense(2, activation='tanh'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def gan_discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=2, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    model.add(LeakyReLU(0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "    return model\n",
    "\n",
    "def define_gan(generator, discriminator):\n",
    "    # connect generator and discriminator\n",
    "    discriminator.trainable = False\n",
    "    ganInput = Input(shape=(input_dim,))\n",
    "    x = generator(ganInput)\n",
    "    ganOutput = discriminator(x)\n",
    "    gan = Model(inputs=ganInput, outputs=ganOutput)\n",
    "    opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return gan\n",
    "\n",
    "\n",
    "generator = gan_generator_model(input_dim)\n",
    "generator.summary()\n",
    "discriminator = gan_discriminator_model()\n",
    "opt = Adam(lr=0.0002, beta_1=0.5)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "discriminator.summary()\n",
    "gan = define_gan(generator, discriminator)\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "half_batch = int(batch_size/2)\n",
    "epochs = 10000\n",
    "\n",
    "test_size = 64\n",
    "half_test = int(test_size /2)\n",
    "\n",
    "errors_discrim = np.zeros((epochs))\n",
    "errors_generator = np.zeros((epochs))\n",
    "perc_discrim = np.zeros((epochs))\n",
    "\n",
    "plt.ion()\n",
    "# manually enumerate epochs\n",
    "for i in range(epochs):\n",
    "    batch = np.zeros((batch_size, 2))\n",
    "    batch_y = np.zeros(batch_size)\n",
    "   \n",
    "    # prepare real samples\n",
    "    random_indices = np.random.choice(len(train_dataset), size=half_batch, replace=False)\n",
    "    batch[:half_batch, :] = train_dataset[random_indices, :]\n",
    "    #Label for the real samples is 0, so no need to change batch_y\n",
    "\n",
    "    batch[half_batch:, :] = generator.predict(np.random.normal(0, 1, size=[half_batch, input_dim]))\n",
    "    batch_y[half_batch:] += 0.9 \n",
    "   \n",
    "        \n",
    "    \n",
    "    discriminator.trainable = True\n",
    "    errors_discrim[i],_ = discriminator.train_on_batch(batch, batch_y)\n",
    "    # create inverted labels for the fake samples\n",
    "    x_gan = np.random.normal(0, 1, size=[batch_size, input_dim])\n",
    "    y_gan = np.zeros((batch_size)) \n",
    "    discriminator.trainable = False\n",
    "    # update the generator via the discriminator's error\n",
    "    errors_generator[i] = gan.train_on_batch(x_gan, y_gan)\n",
    "    \n",
    "    test_batch = np.zeros((test_size, 2))\n",
    "    test_y     = np.zeros(test_size)\n",
    "    test_y[half_test:] += 1.\n",
    "    \n",
    "    random_indices = np.random.choice(len(test_dataset), size=half_test, replace=False)\n",
    "\n",
    "    test_batch[:half_test, :] = test_dataset[random_indices, :]\n",
    "    test_batch[half_test:, :] = generator.predict(np.random.normal(0, 1, size=[half_test, input_dim]))\n",
    "    predictions =  (discriminator.predict(test_batch).ravel() > 0.5) * 1.\n",
    "    \n",
    "    perc_discrim[i] = sum(predictions == test_y)/test_size\n",
    "    \n",
    "    if i%1000 == 0:\n",
    "        print(\"epoch: \"+str(i))\n",
    "        print_images(generator.predict(np.random.normal(0, 1, size=[50, input_dim])))\n",
    "        print(\"Errors Discrimin: \" + str(errors_generator[i]))\n",
    "        print(\"Errors Generator:\" + str(errors_discrim[i]))\n",
    "        print(\"Perc Discrim:\" + str(perc_discrim[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(errors_discrim)\n",
    "\n",
    "plt.plot(errors_generator)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(perc_discrim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = generator.predict(np.random.normal(0, 1, size=[1000, input_dim]))\n",
    "real = generate_samples(10000)\n",
    "\n",
    "plt.scatter(gen[:,0],gen[:,1])\n",
    "plt.scatter(real[:,0], real[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
